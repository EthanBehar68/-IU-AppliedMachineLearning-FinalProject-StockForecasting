Simple RNN:
3 SimpleRNN layers with 32 inputs. Returns full sequence. Drop out of 0.2 after each of these layers.
1 SimpleRNN layer with 32 inputs. Does not return full sequence
1 Dense layer with 1 input. 
input -> 32 -> 32 -> 32 -> 32noFS -> 1
Activation: elu
Kernel Initializer: Glorot Uniform
Recurrent Initializer: Orthogonal
Bias Initializer: Zeros
Optimizer = RMSProp

Groovy RNN:
3 GRU layers with 50 inputs. Returns full sequence. Drop out of 0.2 after each of these layers.
1 GRU layer with 50 inputs. Does not return full sequence
1 Dense layer with 1 input.
input -> 50 -> 50 -> 50 -> 50noFS ->1
Activation: elu
Kernel Initializer: Glorot Uniform
Recurrent Initializer: Orthogonal
Bias Initializer: Zeros
Optimizer = RMSProp

Loovy RNN:
3 LSTM layers with 50 inputs. Returns full sequence. Drop out of 0.2 after each of these layers.
1 LSTM layer with 50 inputs. Does not return full sequence
1 Dense layer with 1 input.
input -> 50 -> 50 -> 50 -> 50noFS ->1
Activation: elu
Kernel Initializer: Glorot Uniform
Recurrent Initializer: Orthogonal
Bias Initializer: Zeros
Optimizer = RMSProp

Super Groovy RNN:
6 GRU layers with 50 inputs. Returns full sequence. Drop out of 0.2 after each of these layers.
1 GRU layer with 50 inputs. Does not return full sequence
1 Dense layer with 1 input.
input -> 50 -> 50 -> 50 -> 50noFS ->1
Activation: elu
Kernel Initializer: Glorot Uniform
Recurrent Initializer: Orthogonal
Bias Initializer: Zeros
Optimizer = RMSProp

Super Loovy RNN:
6 LSTM layers with 50 inputs. Returns full sequence. Drop out of 0.2 after each of these layers.
1 LSTM layer with 50 inputs. Does not return full sequence
1 Dense layer with 1 input.
input -> 50 -> 50 -> 50 -> 50noFS ->1
Activation: elu
Kernel Initializer: Glorot Uniform
Recurrent Initializer: Orthogonal
Bias Initializer: Zeros
Optimizer = RMSProp
