{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd09877b9d49ccc7dbf40aead0404d31c44095faccd031bd07272a835057851d67a",
   "display_name": "Python 3.6.13 64-bit ('p556fp': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "9877b9d49ccc7dbf40aead0404d31c44095faccd031bd07272a835057851d67a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### File Organization ###\n",
    "# Imports\n",
    "# Data Prep Helper methods and variables\n",
    "# Graph Plotting methods\n",
    "# Data Prep - Pulling data from Fastquant\n",
    "# Data Prep - Creating factional data sets\n",
    "# RNN Models\n",
    "    # Simple RNN\n",
    "    # Groovy (5 layer GRU model)\n",
    "    # Loovy (5 layer LSTM model)\n",
    "    # Super Groovy (8 layer GRU model)\n",
    "    # Super Loovy (8 layer LSTM model)\n",
    "# Train/Predict methods\n",
    "# Training/Testing Models\n",
    "    # Simple RNN - High column only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastquant import get_stock_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, GRU, LSTM, Dropout\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep - Helper Methods/Vars\n",
    "\n",
    "day_range = 60 # length of days to split data on\n",
    "\n",
    "# Extract factional change, fractional high, and fractional low\n",
    "# No normalization/standardization\n",
    "def frac_data_prep(data, y_key='close'):\n",
    "    x = pd.DataFrame(data=None, columns=['fracChange','fracHigh','fracLow'])\n",
    "    x['fracChange'] = (data['close']-data['open'])/data['open']\n",
    "    x['fracHigh'] = (data['high']-data['open'])/data['open']\n",
    "    x['fracLow'] = (data['open']-data['low'])/data['open']\n",
    "\n",
    "    # y = data[y_key].values\n",
    "\n",
    "    return x\n",
    "\n",
    "# Probably gonna trash\n",
    "# def get_x_y_frac_sets(ticker):\n",
    "#     if ticker == 'aapl':\n",
    "#         return frac_x_apple_train.to_numpy(), frac_y_apple_train, frac_x_apple_test.to_numpy(), frac_y_apple_test\n",
    "#     if ticker == 'ibm':\n",
    "#         return frac_x_ibm_train.to_numpy(), frac_y_ibm_train, frac_x_ibm_test.to_numpy(), frac_y_ibm_test\n",
    "\n",
    "# Extract high\n",
    "# Normalize high Data\n",
    "def high_data_prep(train_data, test_data, scaler):\n",
    "    # df_train = pd.DataFrame(data=None, columns=['high'])\n",
    "    # df_train['high'] = train_data['high']\n",
    "    high_train_data = train_data['high'].values\n",
    "\n",
    "    # df_test = pd.DataFrame(data=None, columns=['high'])\n",
    "    # df_test['high'] = test_data['high']\n",
    "    high_test_data = test_data['high'].values\n",
    "\n",
    "    high_train_data = high_train_data.reshape(-1,1)\n",
    "    high_test_data = high_test_data.reshape(-1,1)\n",
    "\n",
    "    train_scale = scaler.fit_transform(high_train_data)\n",
    "    test_scale = scaler.transform(high_test_data)\n",
    "\n",
    "    return train_scale, test_scale, high_test_data, scaler\n",
    "\n",
    "# Split train/test data into x/y sets\n",
    "# For single column i.e. like high only\n",
    "def get_x_y_sets(train_data, test_data):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "\n",
    "    for i in range(day_range, len(train_data)):\n",
    "        X_train.append(train_data[i-day_range:i, 0])\n",
    "        Y_train.append(train_data[i,0])\n",
    "\n",
    "    x_train, y_train = np.array(X_train), np.array(Y_train)\n",
    "    x_train = np.reshape(x_train, (*x_train.shape, 1))\n",
    "\n",
    "    # Something weird is going on here\n",
    "    X_test = []\n",
    "    for i in range(day_range, len(test_data)):\n",
    "        X_test.append(train_data[i-day_range:i,0])\n",
    "\n",
    "    x_test = np.array(X_test)\n",
    "    x_test = np.reshape(x_test, (*x_test.shape, 1))\n",
    "\n",
    "    return x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting methods\n",
    "def plot_results(preds, actual, title):\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    ax.set_title(title)\n",
    "    time = range(len(preds))\n",
    "    ax.plot(time,preds,color='tab:red',marker='s',markersize=2,linestyle='-',linewidth=1,label='forcast')\n",
    "    ax.plot(time,actual,color='tab:blue',marker='s',markersize=2,linestyle='-',linewidth=1,label='actual')\n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_ylabel('stock price ($)')\n",
    "    ax.set_xticks(np.arange(0,len(preds)+10,10))\n",
    "    ax.set_xlim(0,len(preds)+10)\n",
    "    ax.xaxis.grid(True,ls='--')\n",
    "    ax.yaxis.grid(True,ls='--')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'../imgs/{title}.png', ext='png', bbox_inches=\"tight\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep - Get Data\n",
    "\n",
    "# training with apple feb-10-2003 -> sep-10-2004\n",
    "full_apple_train = get_stock_data(\"AAPL\",\"2003-02-10\",\"2004-09-10\")\n",
    "# testing with apple sep-13-2004 -> jan-21-2005\n",
    "full_apple_test =  get_stock_data(\"AAPL\",\"2004-09-13\",\"2005-01-21\")\n",
    "# training with IBM feb-10-2003 -> sep-10-2004\n",
    "full_ibm_train = get_stock_data(\"IBM\",\"2003-02-10\",\"2004-09-10\")\n",
    "# testing with IBM sep-13-2004 -> jan-21-2005\n",
    "full_ibm_test =  get_stock_data(\"IBM\",\"2004-09-13\",\"2005-01-21\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Prep - Build Train/Test Sets\n",
    "\n",
    "# Prepare Fractional Change/High/Low Data\n",
    "frac_apple_train = frac_data_prep(full_apple_train) #, frac_y_apple_train = frac_data_prep(full_apple_train)\n",
    "frac_apple_test = frac_data_prep(full_apple_test) #, frac_y_apple_test = frac_data_prep(full_apple_test)\n",
    "frac_ibm_train = frac_data_prep(full_ibm_train) #, frac_y_ibm_train = frac_data_prep(full_ibm_train)\n",
    "frac_ibm_test = frac_data_prep(full_ibm_test) #, frac_y_ibm_test = frac_data_prep(full_ibm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SimpleRNN Model\n",
    "\n",
    "# 3 SimpleRNN layers with 32 inputs. Returns full sequence.\n",
    "# 1 SimpleRNN layer with 32 inputs. Does not return full sequence\n",
    "# 1 Dense layer with 1 input. \n",
    "# input -> 32 -> 32 -> 32 -> 32noFS -> 1\n",
    "# Activation: elu\n",
    "# Kernel Initializer: Glorot Uniform\n",
    "# Recurrent Initializer: Orthogonal\n",
    "# Bias Initializer: Zeross\n",
    "# Optimizer = RMSProp\n",
    "\n",
    "sr_learning_rate = 0.001\n",
    "sr_momentum = 0.9\n",
    "sr_optimizer = RMSprop(learning_rate=sr_learning_rate)\n",
    "sr_loss = 'mean_absolute_percentage_error'\n",
    "sr_activation_funct = 'elu'\n",
    "sr_epochs = 100\n",
    "sr_batch_size = 150\n",
    "sr_dropout = 0.2\n",
    "\n",
    "# Builds a new instance of Simple RNN and scaler (if needed)\n",
    "def build_simple_rnn(useDropout = True):\n",
    "    # scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, return_sequences=True, activation=sr_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sr_dropout))\n",
    "    model.add(SimpleRNN(32, return_sequences=True, activation=sr_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sr_dropout))\n",
    "    model.add(SimpleRNN(32, return_sequences=True, activation=sr_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sr_dropout))\n",
    "    model.add(SimpleRNN(32, return_sequences=False, activation=sr_activation_funct))\n",
    "    model.add(Dense(1, activation=sr_activation_funct))\n",
    "\n",
    "    model.compile(optimizer=sr_optimizer, loss=sr_loss)\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Groovy Model\n",
    "\n",
    "# 3 GRU layers with 50 inputs. Returns full sequence\n",
    "# 1 GRU layer with 50 inputs. Does not return full sequence\n",
    "# 1 Dense layer with 1 input.\n",
    "# input -> 50 -> 50 -> 50 -> 50noFS ->1\n",
    "# Activation: elu\n",
    "# Kernel Initializer: Glorot Uniform\n",
    "# Recurrent Initializer: Orthogonal\n",
    "# Bias Initializer: Zeros\n",
    "# Optimizer = RMSProp\n",
    "\n",
    "# Hyperparameters\n",
    "g_learning_rate = 0.001\n",
    "g_momentum = 0.9\n",
    "g_optimizer = RMSprop(learning_rate=g_learning_rate)\n",
    "g_loss = 'mean_absolute_percentage_error'\n",
    "g_activation_funct = 'elu'\n",
    "g_epochs = 100\n",
    "g_batch_size = 150\n",
    "g_dropout = 0.2\n",
    "\n",
    "def build_groovy(useDropout = True):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    # scaler = StandardScaler()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GRU(50, return_sequences=True, activation=g_activation_funct))\n",
    "    if useDropout: model.add(Dropout(g_dropout))\n",
    "    model.add(GRU(50, return_sequences=True, activation=g_activation_funct))\n",
    "    if useDropout: model.add(Dropout(g_dropout))\n",
    "    model.add(GRU(50, return_sequences=True, activation=g_activation_funct))\n",
    "    if useDropout: model.add(Dropout(g_dropout))\n",
    "    model.add(GRU(50, return_sequences=False, activation=g_activation_funct))\n",
    "    model.add(Dense(1, activation=g_activation_funct))\n",
    "\n",
    "    model.compile(optimizer=g_optimizer, loss=g_loss)\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Loovy Model\n",
    "\n",
    "# 3 LSTM layers with 50 inputs. Returns full sequence\n",
    "# 1 LSTM layer with 50 inputs. Does not return full sequence\n",
    "# 1 Dense layer with 1 input.\n",
    "# input -> 50 -> 50 -> 50 -> 50noFS ->1\n",
    "# Activation: elu\n",
    "# Kernel Initializer: Glorot Uniform\n",
    "# Recurrent Initializer: Orthogonal\n",
    "# Bias Initializer: Zeros\n",
    "# Optimizer = RMSProp\n",
    "\n",
    "# Hyperparameters\n",
    "l_learning_rate = 0.001\n",
    "l_momentum = 0.9\n",
    "l_optimizer = RMSprop(learning_rate=l_learning_rate)\n",
    "l_loss = 'mean_absolute_percentage_error'\n",
    "l_activation_funct = 'elu'\n",
    "l_epochs = 100\n",
    "l_batch_size = 150\n",
    "l_dropout = 0.2\n",
    "\n",
    "def build_loovy(useDropout = True):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    # scaler = StandardScaler()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, activation=l_activation_funct))\n",
    "    if useDropout: model.add(Dropout(l_dropout))\n",
    "    model.add(LSTM(50, return_sequences=True, activation=l_activation_funct))\n",
    "    if useDropout: model.add(Dropout(l_dropout))\n",
    "    model.add(LSTM(50, return_sequences=True, activation=l_activation_funct))\n",
    "    if useDropout: model.add(Dropout(l_dropout))\n",
    "    model.add(LSTM(50, return_sequences=False, activation=l_activation_funct))\n",
    "    model.add(Dense(1, activation=l_activation_funct))\n",
    "\n",
    "    model.compile(optimizer=l_optimizer, loss=l_loss)\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SuperGroovy Model\n",
    "\n",
    "# 6 GRU layers with 50 inputs. Returns full sequence\n",
    "# 1 GRU layer with 50 inputs. Does not return full sequence\n",
    "# 1 Dense layer with 1 input.\n",
    "# input -> 50 -> 50 -> 50 -> 50noFS ->1\n",
    "# Activation: elu\n",
    "# Kernel Initializer: Glorot Uniform\n",
    "# Recurrent Initializer: Orthogonal\n",
    "# Bias Initializer: Zeros\n",
    "# Optimizer = RMSProp\n",
    "\n",
    "# Hyperparameters\n",
    "sg_learning_rate = 0.001\n",
    "sg_momentum = 0.9\n",
    "sg_optimizer = RMSprop(learning_rate=sg_learning_rate)\n",
    "sg_loss = 'mean_absolute_percentage_error'\n",
    "sg_activation_funct = 'elu'\n",
    "sg_epochs = 100\n",
    "sg_batch_size = 150\n",
    "sg_dropout = 0.2\n",
    "\n",
    "def build_super_groovy(useDropout = True):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    # scaler = StandardScaler()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GRU(50, return_sequences=True, activation=sg_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sg_dropout))\n",
    "    model.add(GRU(50, return_sequences=True, activation=sg_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sg_dropout))\n",
    "    model.add(GRU(50, return_sequences=True, activation=sg_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sg_dropout))\n",
    "    model.add(GRU(50, return_sequences=True, activation=sg_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sg_dropout))\n",
    "    model.add(GRU(50, return_sequences=True, activation=sg_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sg_dropout))\n",
    "    model.add(GRU(50, return_sequences=True, activation=sg_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sg_dropout))\n",
    "    model.add(GRU(50, return_sequences=False, activation=sg_activation_funct))\n",
    "    model.add(Dense(1, activation=sg_activation_funct))\n",
    "\n",
    "    model.compile(optimizer=sg_optimizer, loss=sg_loss)\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SuperLoovy Model\n",
    "\n",
    "# 6 LSTM layers with 50 inputs. Returns full sequence\n",
    "# 1 LSTM layer with 50 inputs. Does not return full sequence\n",
    "# 1 Dense layer with 1 input.\n",
    "# input -> 50 -> 50 -> 50 -> 50noFS ->1\n",
    "# Activation: elu\n",
    "# Kernel Initializer: Glorot Uniform\n",
    "# Recurrent Initializer: Orthogonal\n",
    "# Bias Initializer: Zeros\n",
    "# Optimizer = RMSProp\n",
    "\n",
    "# Hyperparameters\n",
    "sl_learning_rate = 0.001\n",
    "sl_momentum = 0.9\n",
    "sl_optimizer = RMSprop(learning_rate=sl_learning_rate)\n",
    "sl_loss = 'mean_absolute_percentage_error'\n",
    "sl_activation_funct = 'elu'\n",
    "sl_epochs = 100\n",
    "sl_batch_size = 150\n",
    "sl_dropout = 0.2\n",
    "\n",
    "def build_super_loovy(useDropout = True):\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    # scaler = StandardScaler()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, activation=sl_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sl_dropout))\n",
    "    model.add(LSTM(50, return_sequences=True, activation=sl_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sl_dropout))\n",
    "    model.add(LSTM(50, return_sequences=True, activation=sl_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sl_dropout))\n",
    "    model.add(LSTM(50, return_sequences=True, activation=sl_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sl_dropout))\n",
    "    model.add(LSTM(50, return_sequences=True, activation=sl_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sl_dropout))\n",
    "    model.add(LSTM(50, return_sequences=True, activation=sl_activation_funct))\n",
    "    if useDropout: model.add(Dropout(sl_dropout))\n",
    "    model.add(LSTM(50, return_sequences=False, activation=sl_activation_funct))\n",
    "    model.add(Dense(1, activation=sl_activation_funct))\n",
    "\n",
    "    model.compile(optimizer=sl_optimizer, loss=sl_loss)\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Predict method\n",
    "def train_predict_high(full_train_data, full_test_data, model, scaler, in_epochs, in_batch_size):\n",
    "    scaled_train, scaled_test, raw_test, scaler = high_data_prep(full_train_data, full_test_data, scaler)\n",
    "\n",
    "    x_train, y_train, x_test = get_x_y_sets(scaled_train, scaled_test)\n",
    "\n",
    "    # print(x_train.shape)\n",
    "    # print(y_train.shape)\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    scaled_predictions = model.predict(x_test)\n",
    "    test_predictions = scaler.inverse_transform(scaled_predictions)\n",
    "\n",
    "    # return model, test_predictions, raw_test\n",
    "    return model, scaled_predictions, scaled_test\n",
    "\n",
    "\n",
    "def train_predict_fractionals(train_data, test_data, model, in_epochs, in_batch_size):\n",
    "    x_train, y_train, x_test = get_x_y_sets(train_data, test_data)\n",
    "\n",
    "    # print(x_train.shape)\n",
    "    # print(y_train.shape)\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=in_epochs, batch_size=in_batch_size)\n",
    "\n",
    "    test_predictions = model.predict(x_test)\n",
    "\n",
    "    return model, test_predictions, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Apple\n",
    "print('Starting Simple RNN MAPE - Apple using High')\n",
    "simple_rnn_apple, sr_scaler_apple = build_simple_rnn()\n",
    "simple_rnn_apple, sr_y_preds, sr_test_data = train_predict_high(full_apple_train, full_apple_test, simple_rnn_apple, sr_scaler_apple, sr_epochs, sr_batch_size)\n",
    "sr_diff = len(sr_test_data) - len(sr_y_preds)\n",
    "plot_results(sr_y_preds, sr_test_data[sr_diff:], 'Simple RNN MAPE - Apple using High')\n",
    "\n",
    "print('Starting Groovy RNN MAPE - Apple using High')\n",
    "groovy_rnn_apple, g_scaler_apple = build_groovy()\n",
    "groovy_rnn_apple, g_y_preds, g_test_data = train_predict_high(full_apple_train, full_apple_test, groovy_rnn_apple, g_scaler_apple, g_epochs, g_batch_size)\n",
    "g_diff = len(g_test_data) - len(g_y_preds)\n",
    "plot_results(g_y_preds, g_test_data[g_diff:], 'Groovy MAPE - Apple using High')\n",
    "\n",
    "print('Starting Loovy RNN MAPE - Apple using High')\n",
    "loovy_rnn_apple, l_scaler_apple = build_loovy()\n",
    "loovy_rnn_apple, l_y_preds, l_test_data = train_predict_high(full_apple_train, full_apple_test, loovy_rnn_apple, l_scaler_apple, l_epochs, l_batch_size)\n",
    "l_diff = len(l_test_data) - len(l_y_preds)\n",
    "plot_results(l_y_preds, l_test_data[l_diff:], 'Loovy MAPE - Apple using High')\n",
    "\n",
    "print('Starting Super Groovy RNN MAPE - Apple using High')\n",
    "super_groovy_rnn_apple, sg_scaler_apple = build_super_groovy()\n",
    "super_groovy_rnn_apple, sg_y_preds, sg_test_data = train_predict_high(full_apple_train, full_apple_test, super_groovy_rnn_apple, sg_scaler_apple, sg_epochs, sg_batch_size)\n",
    "sg_diff = len(sg_test_data) - len(sg_y_preds)\n",
    "plot_results(sg_y_preds, sg_test_data[sg_diff:], 'Super Groovy MAPE - Apple using High')\n",
    "\n",
    "print('Starting Super Loovy RNN MAPE - Apple using High')\n",
    "super_loovy_rnn_apple, sl_scaler_apple = build_super_loovy()\n",
    "super_loovy_rnn_apple, sl_y_preds, sl_test_data = train_predict_high(full_apple_train, full_apple_test, super_loovy_rnn_apple, sl_scaler_apple, sl_epochs, sl_batch_size)\n",
    "sl_diff = len(sl_test_data) - len(sl_y_preds)\n",
    "plot_results(sl_y_preds, sl_test_data[sl_diff:], 'Super Loovy MAPE - Apple using High')\n",
    "\n",
    "# IBM\n",
    "print('Starting Simple RNN MAPE - IBM using High')\n",
    "simple_rnn_ibm, sr_scaler_ibm = build_simple_rnn()\n",
    "simple_rnn_ibm, sr_y_preds, sr_test_data = train_predict_high(full_ibm_train, full_ibm_test, simple_rnn_ibm, sr_scaler_ibm, sr_epochs, sr_batch_size)\n",
    "sr_diff = len(sr_test_data) - len(sr_y_preds)\n",
    "plot_results(sr_y_preds, sr_test_data[sr_diff:], 'Simple RNN MAPE - IBM using High')\n",
    "\n",
    "print('Starting Groovy RNN MAPE - IBM using High')\n",
    "groovy_rnn_ibm, g_scaler_ibm = build_groovy()\n",
    "groovy_rnn_ibm, g_y_preds, g_test_data = train_predict_high(full_ibm_train, full_ibm_test, groovy_rnn_ibm, g_scaler_ibm, g_epochs, g_batch_size)\n",
    "g_diff = len(g_test_data) - len(g_y_preds)\n",
    "plot_results(g_y_preds, g_test_data[g_diff:], 'Groovy MAPE - IBM using High')\n",
    "\n",
    "print('Starting Loovy RNN MAPE - IBM using High')\n",
    "loovy_rnn_ibm, l_scaler_ibm = build_loovy()\n",
    "loovy_rnn_ibm, l_y_preds, l_test_data = train_predict_high(full_ibm_train, full_ibm_test, loovy_rnn_ibm, l_scaler_ibm, l_epochs, l_batch_size)\n",
    "l_diff = len(l_test_data) - len(l_y_preds)\n",
    "plot_results(l_y_preds, l_test_data[l_diff:], 'Loovy MAPE - IBM using High')\n",
    "\n",
    "print('Starting Super Groovy RNN MAPE - IBM using High')\n",
    "super_groovy_rnn_ibm, sg_scaler_ibm = build_super_groovy()\n",
    "super_groovy_rnn_ibm, sg_y_preds, sg_test_data = train_predict_high(full_ibm_train, full_ibm_test, super_groovy_rnn_ibm, sg_scaler_ibm, sg_epochs, sg_batch_size)\n",
    "sg_diff = len(sg_test_data) - len(sg_y_preds)\n",
    "plot_results(sg_y_preds, sg_test_data[sg_diff:], 'Super Groovy MAPE - IBM using High')\n",
    "\n",
    "print('Starting Super Loovy RNN MAPE - IBM using High')\n",
    "super_loovy_rnn_ibm, sl_scaler_ibm = build_super_loovy()\n",
    "super_loovy_rnn_ibm, sl_y_preds, sl_test_data = train_predict_high(full_ibm_train, full_ibm_test, super_loovy_rnn_ibm, sl_scaler_ibm, sl_epochs, sl_batch_size)\n",
    "sl_diff = len(sl_test_data) - len(sl_y_preds)\n",
    "plot_results(sl_y_preds, sl_test_data[sl_diff:], 'Super Loovy MAPE - IBM using High')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_rnn_fapple, sr_scaler_apple = build_simple_rnn()\n",
    "# simple_rnn_fapple, y_preds, y_test = train_predict_fractionals(frac_apple_train, frac_apple_test, simple_rnn_fapple, sr_epochs, sr_batch_size)\n",
    "# diff = len(test_data) - len(y_preds)\n",
    "# plot_results(y_preds, y_test, 'Simple RNN MAPE - Apple using Fractionals')\n",
    "\n",
    "# simple_rnn_fibm, sr_scaler_apple = build_simple_rnn()\n",
    "# simple_rnn_fibm, y_preds, test_data = train_predict_fractionals('ibm', simple_rnn_fibm, sr_epochs, sr_batch_size)\n",
    "# # diff = len(test_data) - len(y_preds)\n",
    "# plot_results(y_preds, y_test, 'Simple RNN MAPE - IBM using Fractionals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sl_diff = len(sl_test_data) - len(sl_y_preds)\n",
    "# print(sl_diff)\n",
    "# plot_results(sl_y_preds, sl_test_data[sl_diff:], 'Super Loovy MAPE - IBM using High')\n",
    "# print(sl_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}